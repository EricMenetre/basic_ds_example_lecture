{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CONNECT TO WIFI**  \n",
    "SSID: NIDS_course  \n",
    "Password: reproduciblescience  \n",
    "\n",
    "**START VNC VIEWER** (Remmina on Ubuntu, TigerVNC on other OS)  \n",
    "Use IP address on piece of paper  \n",
    "Password: bigbrain  \n",
    "\n",
    "ANY PROBLEM? Please put on your screen a:\n",
    "* yellow post-its for VM / wifi issues\n",
    "* pink post-its for course content questions\n",
    "\n",
    "**LOGIN ON THE VM**  \n",
    "Login (pre-set): brainhacker  \n",
    "Password: brainhack!  \n",
    "\n",
    "Please:\n",
    "* connect to VM\n",
    "* start Chrome\n",
    "* fill form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Full Basic Data Science Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Conda\n",
    "Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"conda_illustration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Common conda commands:\n",
    "* Create a new environment named sci35, install Python 3.5: `conda create --name sci35 python=3.5`\n",
    "* Activate the conda environment sci35: `conda activate sci35`\n",
    "* Then installing package `my_pack`: `conda install my_pack`\n",
    "* Updating package `outdated_pack`: `conda update outdated_pack`\n",
    "* Listing packages in current env: `conda list`\n",
    "* Exiting current environment: `conda deactivate`\n",
    "* List all environments: `conda env list`\n",
    "* Export an environment to a file (first need to activate it): `conda env export --from-history > my_env.yml`\n",
    "* Create an environment from a file: `conda env create -f my_env.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Science Example on World Happiness (WH) Dataset\n",
    "\n",
    "You will work with:\n",
    "* Conda\n",
    "* Git\n",
    "* Jupyter notebook\n",
    "* Visual Studio (VS) Code\n",
    "* Python libraries pandas and scikit-learn (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conda tasks [part 1 / 5]\n",
    "1. Create a new conda environment based on python 3.7 with the name of your choosing\n",
    "2. Activate that environment\n",
    "3. Install `jupyter` and `nb_conda_kernels` in that environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### END OF PART 1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Git tasks [part 2 / 5]\n",
    "**Work by group of 1 to 4 with `git` to:**\n",
    "1. Have one of your group create a repo on her/his Github account (step 1 to 8 [here](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-new-repository) and add group members as collaborators (cf [here](https://help.github.com/en/github/setting-up-and-managing-your-github-user-account/inviting-collaborators-to-a-personal-repository)). You can name your repo whatever you want, for example `WH-analysis`.\n",
    "2. Have each member create the directory `WH-project` in their home directory `/home/brainhacker`\n",
    "3. Have each member clone the repo from step 1 locally inside `/home/brainhacker/WH-project` (cf step 1 to 7 [here](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Each member can now work on his local version (TIP: have members work on different files to avoid merge conflicts)\n",
    "  1. You can update with the latest remote version using `git fetch origin` and then `git merge origin/master` (cf [here](https://help.github.com/en/github/using-git/getting-changes-from-a-remote-repository))\n",
    "  2. You can then add, commit and push your changes to the remote with `git push origin master` (cf [here](https://help.github.com/en/github/managing-files-in-a-repository/adding-a-file-to-a-repository-using-the-command-line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. The reference repo `github.com/mick-d/NIDS1920-WH-data` contains the train and test data and is the place where you will need to send your happiness prediction via pull-request at the end of the workshop. \n",
    "  1. Have one of your member fork this repo to her/his Github account repo (`github.com/[github_username]/NIDS1920-WH-data`) and then clone that repo (cf [all the steps here](https://help.github.com/en/github/getting-started-with-github/fork-a-repo)) in her/his directory `/home/brainhacker/WH-project`.\n",
    "  2. Have that same group member copy the train and test data to the analysis repo you created earlier (`WH-analysis` if you named it that way) and push the change to her/his analysis remote origin (`git add`, `git commit`, `git push`)\n",
    "  3. All members should update their local repo to have the raw data with `git pull`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"git_setup.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### END OF PART 2 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jupyter/VS Code/Pandas tasks [part 3 / 5]:\n",
    "1. Start a terminal, and make sure you are in your home dir (you can type `cd` to change to your home dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Run `jupyter notebook` to create a new Jupyter notebook:\n",
    "  1. In the new tab appearing in your browser, go inside `WH-project` within the folder hierarchy\n",
    "  2. Click `File --> New Notebook` and choose the conda environment you created in the previous task series\n",
    "  3. Once opened, rename your Notebook by clicking on \"Untitled\" on the top left, and choose a name without space characters (it will be easier to manipulate the notebook from the command-line later on).\n",
    "3. Use the `pandas` library to read the CSV file containing the raw data into a Pandas Dataframe (cf first cells of \"Data analysis with pandas\" in python_intro notebook at `~/python_lecture/python_intro.ipynb`)  \n",
    "TIP: `pandas` needs to be already installed in the conda environment (if not done already, then install it) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Create a function which has `path_to_WH_data` and `year` as arguments, and return a dataframe with **only WH data belonging to that year** and **only columns ranging from `Life ladder` to `Perceptions of corruption`** (cf python_intro notebook for function)\n",
    "  1. TIP 1: you can select rows and columns with the `.loc` method, as in `selec_df = orig_df.loc[selec_rows, selec_columns]`\n",
    "  2. TIP 2: selected_rows is typically a boolean series which can be obtained from comparison operators applied to columns, as in `selec_rows = orig_df[\"weight\"] < 100` (boolean operators include `<`, `<=`, `==`, `>=`, `>`)\n",
    "  3. TIP 3: selected_columns is typically a list of column names as in `[\"firstname\", \"surname\", \"age\"]`. You can obtain the list of column names of a dataframe with `.columns`, as in `my_cols = orig_df.columns`. You can then select whatever columns you want from this list using slicing for example (`selec_columns = my_cols[2:5]`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Have one member start VS code and:\n",
    "  1. Create a new Python file inside the local `WH-analysis` repo (e.g. `/home/brainhacker/WH-project/WH-analysis/WH_utils.py`)\n",
    "    1. `File --> New File` then `File --> Save As` (don't forget .py extension, e.g. `WH_utils.py`)\n",
    "    2. Select your conda environment by pressing `Ctrl+Shift+P` and click on `Python: Select Interpreter`. Now a list of conda environment suggestions will appear. Start typing the name of the environment you created and when found click on it. For more info, cf [here](https://code.visualstudio.com/docs/python/environments)\n",
    "    3. If proposed by VS Code you can install `pylint` which offers useful highlighting and information when typing python code. \n",
    "  2. Write in that file the function you defined earlier in the notebook\n",
    "  3. Push the saved file to the remote origin repo (`git add`, `git commit`, `git push`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "6. If more than one member, have all other members pull the remote origin repo locally to get the new python file in their local repo\n",
    "7. In your Jupyter notebook:\n",
    "  1. Delete the cell containing the function\n",
    "  2. Add the path of the directory containing the function to your system path by typing the following in a new cell:\n",
    "    * `import sys`\n",
    "    * `sys.path.append('the-name-of-your-dir')`\n",
    "  3. Now use instead the function you wrote in the file `WH_utils.py` by importing it as a module (`import WH_utils`).\n",
    "  3. Test the imported function work as expected (`WH_utils.my_function_name(...)`) by creating a dataframe for WH data for the year 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### END OF PART 3 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandas / matplotlib / seaborn tasks [part 4 / 5]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Working on the WH data for the year 2018 (please refer to \"Data analysis with pandas\" section of `python_intro` notebook for all tasks below if you require help)\n",
    "1. Examine summary statistics of your dataframe using the `.describe` method\n",
    "2. Count the number of rows in your dataframe using the `.shape` attribute\n",
    "3. Count the number of missing values (`NA` for \"not available\") using `.isna` and `.sum` methods\n",
    "4. Create a new \"cleaned\" dataframe by removing all rows with `NA` values using the `.dropna` method\n",
    "5. Count the new number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "6. To do some plotting, import `seaborn` and `pyplot` from `matplotlib` with conventional shortcuts in the field (cf `python_intro` notebook). \n",
    "7. Use the magic command `%matplotlib inline` in a cell to make sure all your plots will appear in the notebook\n",
    "8. Look at the distribution of your dataframe columns data with histograms using the `.hist` method. Use the help to find out how to set the figure size (remember the size should be given as a tuple such as (12,6)).  \n",
    "TIP: Use the `plt.tight_layout()` command to arrange overlapping issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "9. Use matplotlib `scatter` method (cf \"Numerical analysis and plotting with numpy and matplotlib\" section of `python_intro`) to plot the `Life Ladder` column as a function of `Freedom to make life choices`\n",
    "10. See how `seaborn` shines by using its `jointplot` method, and then its `regplot` method. For arguments use `x` and `y` for the name of the two columns you want to plot, and `data` for the name of your dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### END OF PART 4 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandas / scikit-learn tasks [part 5 / 5]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scikit-learn is a machine learning python package. You will use a simple multiple linear regression (MLR) model to train your model, and then test it on unseen data.\n",
    "<img src=\"multiple_linear.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With scikit-learn you can fit a MLR model (i.e. calculate the beta regressors) between the response variable y (also called DV) and the predictors X (also calleds IVs) in a few lines:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_seen, y_seen) # the betas are now computed and you can use your model on any new X to predict y\n",
    "y_unseen = lm.predict(X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Train (i.e. fit) your MLR model on the WH train data for the year 2018:\n",
    "1. Create the variable `X` by removing the column \"Life Ladder\" (use method `.drop` with argument `column=\"Life Ladder\"`)\n",
    "2. Create the variable `y` by setting it to the column \"Life Ladder\"\n",
    "3. Use the `.fit` method to train your model\n",
    "4. Use your trained model to predict the value of \"Life Ladder\" for the WH test data for the year 2018\n",
    "5. Add the predicted column to the WH test dataframe (with only rows for year 2018 and columns of interest), and save the result (use method `.tocsv` with argument `index=False`)\n",
    "6. Have one member of your group push the result to her/his forked `NIDS1920-WH-data` repo, and submit a pull-request to the upstream repo as described [here](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request-from-a-fork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "7. Write (in VS Code) a function which has the path of WH train data, the path of WH test data and the year as arguments, and return the path to a new CSV file corresponding to WH test data for that year with a new column including the happiness score predictions.\n",
    "8. Have one member of your group push the new function to your analysis repo.\n",
    "9. Export your conda environment into a file, and push that file to the repo\n",
    "10. Have one member of your group clean their Jupyter notebook and copy it to your local analysis repo, then pushing the changes to the remote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### END OF PART 5 ! You now have a repo with all your analysis code, together with an illustrative notebook, and you contributed to an external repo by submitting a pull-request with your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BONUS [PART 6 / 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can calculate the R squared of your MLR model with the `score` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R2_seendata = lm.score(X_seen, y_seen)\n",
    "R2_real = lm.score(X_unseen, y_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do you expect `R2_seendata` to compare with `R2_real`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using cross-validation allows to estimate the generalization of the results, i.e how the model would behave with new data. You can implement simply K-fold cross-validation in scikit-learn with `KFold` from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cross_validation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sci37] *",
   "language": "python",
   "name": "conda-env-sci37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
